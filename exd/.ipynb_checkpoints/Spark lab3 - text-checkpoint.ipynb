{
 "metadata": {
  "name": "",
  "signature": "sha256:7b18f7f917e747e82398ba6fa11252d422633336ac6788cbf1d40d9ce1a240c1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "DATAFILE_PATTERN = '^(.+),\"(.+)\",(.*),(.*),(.*)'\n",
      "\n",
      "def removeQuotes(s):\n",
      "    \"\"\" Remove quotation marks from an input string\n",
      "    Args:\n",
      "        s (str): input string that might have the quote \"\" characters\n",
      "    Returns:\n",
      "        str: a string without the quote characters\n",
      "    \"\"\"\n",
      "    return ''.join(i for i in s if i!='\"')\n",
      "\n",
      "\n",
      "def parseDatafileLine(datafileLine):\n",
      "    \"\"\" Parse a line of the data file using the specified regular expression pattern\n",
      "    Args:\n",
      "        datafileLine (str): input string that is a line from the data file\n",
      "    Returns:\n",
      "        str: a string parsed using the given regular expression and without the quote characters\n",
      "    \"\"\"\n",
      "    match = re.search(DATAFILE_PATTERN, datafileLine)\n",
      "    if match is None:\n",
      "        print 'Invalid datafile line: %s' % datafileLine\n",
      "        return (datafileLine, -1)\n",
      "    elif match.group(1) == '\"id\"':\n",
      "        print 'Header datafile line: %s' % datafileLine\n",
      "        return (datafileLine, 0)\n",
      "    else:\n",
      "        product = '%s %s %s' % (match.group(2), match.group(3), match.group(4))\n",
      "        return ((removeQuotes(match.group(1)), product), 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import os\n",
      "\n",
      "\n",
      "baseDir = os.path.join('data')\n",
      "inputPath = os.path.join('cs100', 'lab3')\n",
      "\n",
      "GOOGLE_PATH = 'Google.csv'\n",
      "GOOGLE_SMALL_PATH = 'Google_small.csv'\n",
      "AMAZON_PATH = 'Amazon.csv'\n",
      "AMAZON_SMALL_PATH = 'Amazon_small.csv'\n",
      "GOLD_STANDARD_PATH = 'Amazon_Google_perfectMapping.csv'\n",
      "STOPWORDS_PATH = 'stopwords.txt'\n",
      "\n",
      "def parseData(filename):\n",
      "    \"\"\" Parse a data file\n",
      "    Args:\n",
      "        filename (str): input file name of the data file\n",
      "    Returns:\n",
      "        RDD: a RDD of parsed lines\n",
      "    \"\"\"\n",
      "    return (sc\n",
      "            .textFile(filename, 4, 0)\n",
      "            .map(parseDatafileLine)\n",
      "            .cache())\n",
      "\n",
      "def loadData(path):\n",
      "    \"\"\" Load a data file\n",
      "    Args:\n",
      "        path (str): input file name of the data file\n",
      "    Returns:\n",
      "        RDD: a RDD of parsed valid lines\n",
      "    \"\"\"\n",
      "    filename = os.path.join(baseDir, inputPath, path)\n",
      "    raw = parseData(filename).cache()\n",
      "    failed = (raw\n",
      "              .filter(lambda s: s[1] == -1)\n",
      "              .map(lambda s: s[0]))\n",
      "    for line in failed.take(10):\n",
      "        print '%s - Invalid datafile line: %s' % (path, line)\n",
      "    valid = (raw\n",
      "             .filter(lambda s: s[1] == 1)\n",
      "             .map(lambda s: s[0])\n",
      "             .cache())\n",
      "    print '%s - Read %d lines, successfully parsed %d lines, failed to parse %d lines' % (path,\n",
      "                                                                                        raw.count(),\n",
      "                                                                                        valid.count(),\n",
      "                                                                                        failed.count())\n",
      "    assert failed.count() == 0\n",
      "    assert raw.count() == (valid.count() + 1)\n",
      "    return valid\n",
      "\n",
      "googleSmall = loadData(GOOGLE_SMALL_PATH)\n",
      "google = loadData(GOOGLE_PATH)\n",
      "amazonSmall = loadData(AMAZON_SMALL_PATH)\n",
      "amazon = loadData(AMAZON_PATH)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Google_small.csv - Read 201 lines, successfully parsed 200 lines, failed to parse 0 lines\n",
        "Google.csv - Read 3227 lines, successfully parsed 3226 lines, failed to parse 0 lines"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Amazon_small.csv - Read 201 lines, successfully parsed 200 lines, failed to parse 0 lines"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Amazon.csv - Read 1364 lines, successfully parsed 1363 lines, failed to parse 0 lines"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for line in googleSmall.take(3):\n",
      "    print 'google: %s: %s\\n' % (line[0], line[1])\n",
      "\n",
      "for line in amazonSmall.take(3):\n",
      "    print 'amazon: %s: %s\\n' % (line[0], line[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "google: http://www.google.com/base/feeds/snippets/11448761432933644608: spanish vocabulary builder \"expand your vocabulary! contains fun lessons that both teach and entertain you'll quickly find yourself mastering new terms. includes games and more!\" \n",
        "\n",
        "google: http://www.google.com/base/feeds/snippets/8175198959985911471: topics presents: museums of world \"5 cd-rom set. step behind the velvet rope to examine some of the most treasured collections of antiquities art and inventions. includes the following the louvre - virtual visit 25 rooms in full screen interactive video detailed map of the louvre ...\" \n",
        "\n",
        "google: http://www.google.com/base/feeds/snippets/18445827127704822533: sierrahome hse hallmark card studio special edition win 98 me 2000 xp \"hallmark card studio special edition (win 98 me 2000 xp)\" \"sierrahome\"\n",
        "\n",
        "amazon: b000jz4hqo: clickart 950 000 - premier image pack (dvd-rom)  \"broderbund\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "amazon: b0006zf55o: ca international - arcserve lap/desktop oem 30pk \"oem arcserve backup v11.1 win 30u for laptops and desktops\" \"computer associates\"\n",
        "\n",
        "amazon: b00004tkvy: noah's ark activity center (jewel case ages 3-8)  \"victory multimedia\"\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "test_string1 = 'A quick brown fox jumps over the lazy dog.'\n",
      "test_string2 = '!!!!123A/456_B/789C.123A'\n",
      "split_regex = r'\\W+'\n",
      "\n",
      "def simpleTokenize(string):\n",
      "    return re.findall(r'\\w+', string.strip().lower())\n",
      "\n",
      "print simpleTokenize(test_string1)\n",
      "print simpleTokenize(test_string2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['a', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
        "['123a', '456_b', '789c', '123a']\n"
       ]
      }
     ],
     "prompt_number": 82
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stopfile = os.path.join(baseDir, inputPath, STOPWORDS_PATH)\n",
      "stopwords = set(sc.textFile(stopfile).collect())\n",
      "print 'These are the stopwords: %s' % stopwords\n",
      "\n",
      "def tokenize(string):\n",
      "    words = simpleTokenize(string)\n",
      "    return filter(lambda x: x not in stopwords, words)\n",
      "\n",
      "print tokenize(quickbrownfox)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "These are the stopwords: set([u'all', u'just', u'being', u'over', u'both', u'through', u'yourselves', u'its', u'before', u'with', u'had', u'should', u'to', u'only', u'under', u'ours', u'has', u'do', u'them', u'his', u'very', u'they', u'not', u'during', u'now', u'him', u'nor', u'did', u'these', u't', u'each', u'where', u'because', u'doing', u'theirs', u'some', u'are', u'our', u'ourselves', u'out', u'what', u'for', u'below', u'does', u'above', u'between', u'she', u'be', u'we', u'after', u'here', u'hers', u'by', u'on', u'about', u'of', u'against', u's', u'or', u'own', u'into', u'yourself', u'down', u'your', u'from', u'her', u'whom', u'there', u'been', u'few', u'too', u'themselves', u'was', u'until', u'more', u'himself', u'that', u'but', u'off', u'herself', u'than', u'those', u'he', u'me', u'myself', u'this', u'up', u'will', u'while', u'can', u'were', u'my', u'and', u'then', u'is', u'in', u'am', u'it', u'an', u'as', u'itself', u'at', u'have', u'further', u'their', u'if', u'again', u'no', u'when', u'same', u'any', u'how', u'other', u'which', u'you', u'who', u'most', u'such', u'why', u'a', u'don', u'i', u'having', u'so', u'the', u'yours', u'once'])\n"
       ]
      },
      {
       "ename": "NameError",
       "evalue": "name 'quickbrownfox' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-7-713c161f7406>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquickbrownfox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mNameError\u001b[0m: name 'quickbrownfox' is not defined"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "amazonRecToToken = amazonSmall.map(lambda line: (line[0], tokenize(line[1])))\n",
      "googleRecToToken = googleSmall.map(lambda line: (line[0], tokenize(line[1])))\n",
      "\n",
      "def countTokens(vendorRDD):\n",
      "    \"\"\" Count and return the number of tokens\n",
      "    Args:\n",
      "        vendorRDD (RDD of (recordId, tokenizedValue)): Pair tuple of record ID to tokenized output\n",
      "    Returns:\n",
      "        count: count of all tokens\n",
      "    \"\"\"\n",
      "    return vendorRDD.map(lambda raw: len(raw[1])).reduce(lambda x, y: x+y)\n",
      "    \n",
      "\n",
      "totalTokens = countTokens(amazonRecToToken) + countTokens(googleRecToToken)\n",
      "print 'There are %s tokens in the combined datasets' % totalTokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are 22520 tokens in the combined datasets\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def findBiggestRecord(vendorRDD):\n",
      "    \"\"\" Find and return the record with the largest number of tokens\n",
      "    Args:\n",
      "        vendorRDD (RDD of (recordId, tokens)): input Pair Tuple of record ID and tokens\n",
      "    Returns:\n",
      "        list: a list of 1 Pair Tuple of record ID and tokens\n",
      "    \"\"\"\n",
      "    return vendorRDD.map(lambda raw: (raw[0], len(raw[1]))).reduceByKey(lambda x, y: x+y).takeOrdered(1, key = lambda x: -x[1])\n",
      "\n",
      "biggestRecordAmazon = findBiggestRecord(amazonRecToToken)\n",
      "print biggestRecordAmazon\n",
      "print 'The Amazon record with ID \"%s\" has the most tokens (%s)' % (biggestRecordAmazon[0][0],\n",
      "                                                                   biggestRecordAmazon[0][1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('b000o24l3q', 1547)]\n",
        "The Amazon record with ID \"b000o24l3q\" has the most tokens (1547)\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import defaultdict\n",
      "\n",
      "def tf(tokens):\n",
      "    \"\"\" Compute TF\n",
      "    Args:\n",
      "        tokens (list of str): input list of tokens from tokenize\n",
      "    Returns:\n",
      "        dictionary: a dictionary of tokens to its TF values\n",
      "    \"\"\"\n",
      "    d = defaultdict(int)\n",
      "    for token in tokens:\n",
      "        d[token] += 1\n",
      "    tatal_count = len(tokens)\n",
      "    for (k, v) in d.iteritems():\n",
      "        d[k] = v/float(tatal_count)\n",
      "    return d\n",
      "\n",
      "print tf(tokenize(test_string1)) # Should give { 'quick': 0.1666 ... }"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "defaultdict(<type 'int'>, {'brown': 0.16666666666666666, 'lazy': 0.16666666666666666, 'jumps': 0.16666666666666666, 'fox': 0.16666666666666666, 'dog': 0.16666666666666666, 'quick': 0.16666666666666666})\n"
       ]
      }
     ],
     "prompt_number": 83
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpusRDD = googleSmall.cogroup(amazonSmall)\n",
      "corpusRDD.count()\n",
      "corpusRDD.take(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "[('http://www.google.com/base/feeds/snippets/18404187604346548446',\n",
        "  (<pyspark.resultiterable.ResultIterable at 0x7f3c6da32690>,\n",
        "   <pyspark.resultiterable.ResultIterable at 0x7f3c6dae6ed0>)),\n",
        " ('http://www.google.com/base/feeds/snippets/18436702590661313778',\n",
        "  (<pyspark.resultiterable.ResultIterable at 0x7f3c6dae6c50>,\n",
        "   <pyspark.resultiterable.ResultIterable at 0x7f3c6dae68d0>)),\n",
        " ('b0009stm6g',\n",
        "  (<pyspark.resultiterable.ResultIterable at 0x7f3c6dae6850>,\n",
        "   <pyspark.resultiterable.ResultIterable at 0x7f3c6dae6bd0>)),\n",
        " ('b0002e3g6o',\n",
        "  (<pyspark.resultiterable.ResultIterable at 0x7f3c6dae6950>,\n",
        "   <pyspark.resultiterable.ResultIterable at 0x7f3c6dae6cd0>)),\n",
        " ('http://www.google.com/base/feeds/snippets/18405745276041721942',\n",
        "  (<pyspark.resultiterable.ResultIterable at 0x7f3c6dae6c90>,\n",
        "   <pyspark.resultiterable.ResultIterable at 0x7f3c6dae59d0>)),\n",
        " ('b0000ycfcw',\n",
        "  (<pyspark.resultiterable.ResultIterable at 0x7f3c6dae5110>,\n",
        "   <pyspark.resultiterable.ResultIterable at 0x7f3c6dae51d0>)),\n",
        " ('b000kmcf0g',\n",
        "  (<pyspark.resultiterable.ResultIterable at 0x7f3c6dae5510>,\n",
        "   <pyspark.resultiterable.ResultIterable at 0x7f3c6dae5590>)),\n",
        " ('b000qv5xqy',\n",
        "  (<pyspark.resultiterable.ResultIterable at 0x7f3c6dae5490>,\n",
        "   <pyspark.resultiterable.ResultIterable at 0x7f3c6dae50d0>)),\n",
        " ('http://www.google.com/base/feeds/snippets/8804538687114603592',\n",
        "  (<pyspark.resultiterable.ResultIterable at 0x7f3c6dae5290>,\n",
        "   <pyspark.resultiterable.ResultIterable at 0x7f3c6dae5610>)),\n",
        " ('http://www.google.com/base/feeds/snippets/10684672402205616674',\n",
        "  (<pyspark.resultiterable.ResultIterable at 0x7f3c6dae54d0>,\n",
        "   <pyspark.resultiterable.ResultIterable at 0x7f3c6dae53d0>))]"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from operator import add\n",
      "\n",
      "def idfs(corpus):\n",
      "    \"\"\" Compute IDF\n",
      "    Args:\n",
      "        corpus (RDD): input corpus\n",
      "    Returns:\n",
      "        RDD: a RDD of (token, IDF value)\n",
      "    \"\"\"\n",
      "    N = corpus.keys().count()\n",
      "    tokenSumPairTuple = corpus.map(lambda x: [(word, 1) for word in set(x[1])]).flatMap(lambda x:x).reduceByKey(lambda x, y: x + y)\n",
      "    return tokenSumPairTuple.mapValues(lambda x: N/float(x))\n",
      "\n",
      "idfsSmall = idfs(amazonRecToToken.union(googleRecToToken))\n",
      "idfsSmall.takeOrdered(1, lambda s: s[1])[0]\n",
      "\n",
      "# print 'There are %s unique tokens in the small datasets.' % uniqueTokenCount"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 70,
       "text": [
        "('software', 4.25531914893617)"
       ]
      }
     ],
     "prompt_number": 70
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "smallIDFTokens = idfsSmall.takeOrdered(11, lambda s: s[1])\n",
      "print smallIDFTokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('software', 4.25531914893617), ('new', 6.896551724137931), ('features', 6.896551724137931), ('use', 7.017543859649122), ('complete', 7.2727272727272725), ('easy', 7.6923076923076925), ('create', 8.333333333333334), ('system', 8.333333333333334), ('cd', 8.333333333333334), ('1', 8.51063829787234), ('windows', 8.51063829787234)]\n"
       ]
      }
     ],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "\n",
      "small_idf_values = idfsSmall.map(lambda s: s[1]).collect()\n",
      "fig = plt.figure(figsize=(8,3))\n",
      "plt.hist(small_idf_values, 50, log=True)\n",
      "pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAewAAADLCAYAAACs9KBaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7f3c6da3d850>"
       ]
      }
     ],
     "prompt_number": 73
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tfidf(tokens, idfs):\n",
      "    \"\"\" Compute TF-IDF\n",
      "    Args:\n",
      "        tokens (list of str): input list of tokens from tokenize\n",
      "        idfs (dictionary): record to IDF value\n",
      "    Returns:\n",
      "        dictionary: a dictionary of records to TF-IDF values\n",
      "    \"\"\"\n",
      "    tfs = tf(tokens)\n",
      "    return {k: idfs.get(k, 1) * v for k, v in tfs.iteritems()}\n",
      "\n",
      "recb000hkgj8k = amazonRecToToken.filter(lambda x: x[0] == 'b000hkgj8k').collect()[0][1]\n",
      "print recb000hkgj8k\n",
      "idfsSmallWeights = idfsSmall.collectAsMap()\n",
      "rec_b000hkgj8k_weights = tfidf(recb000hkgj8k, idfsSmallWeights)\n",
      "print 'Amazon record \"b000hkgj8k\" has tokens and weights:\\n%s' % rec_b000hkgj8k_weights"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['autocad', '2007', 'courseware', 'customizing', 'interface', 'autocad', '2007', 'courseware', 'customizing', 'interface', 'autodesk', 'psg']\n",
        "Amazon record \"b000hkgj8k\" has tokens and weights:\n",
        "{'autocad': 33.33333333333333, 'autodesk': 8.333333333333332, 'courseware': 66.66666666666666, 'psg': 33.33333333333333, '2007': 3.5087719298245617, 'customizing': 16.666666666666664, 'interface': 3.0303030303030303}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 110
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import math\n",
      "\n",
      "def dotprod(a, b):\n",
      "    \"\"\" Compute dot product\n",
      "    Args:\n",
      "        a (dictionary): first dictionary of record to value\n",
      "        b (dictionary): second dictionary of record to value\n",
      "    Returns:\n",
      "        dotProd: result of the dot product with the two input dictionaries\n",
      "    \"\"\"\n",
      "    c = a.keys() & b.keys()\n",
      "    return sum(a[k] * b[k] for k in a.keys())\n",
      "        \n",
      "def norm(a):\n",
      "    \"\"\" Compute square root of the dot product\n",
      "    Args:\n",
      "        a (dictionary): a dictionary of record to value\n",
      "    Returns:\n",
      "        norm: a dictionary of tokens to its TF values\n",
      "    \"\"\"\n",
      "    return sqrt(sum(v*v for v in a.values()))\n",
      "\n",
      "def cossim(a, b):\n",
      "    \"\"\" Compute cosine similarity\n",
      "    Args:\n",
      "        a (dictionary): first dictionary of record to value\n",
      "        b (dictionary): second dictionary of record to value\n",
      "    Returns:\n",
      "        cossim: dot product of two dictionaries divided by the norm of the first dictionary and\n",
      "                then by the norm of the second dictionary\n",
      "    \"\"\"\n",
      "    return dotprod(a, b) / (norm(a) * norm(b))\n",
      "\n",
      "testVec1 = {'foo': 2, 'bar': 3, 'baz': 5 }\n",
      "testVec2 = {'foo': 1, 'bar': 0, 'baz': 20 }\n",
      "dp = dotprod(testVec1, testVec2)\n",
      "nm = norm(testVec1)\n",
      "print dp, nm\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "102 6.16441400297\n"
       ]
      }
     ],
     "prompt_number": 106
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def cosineSimilarity(string1, string2, idfsDictionary):\n",
      "    \"\"\" Compute cosine similarity between two strings\n",
      "    Args:\n",
      "        string1 (str): first string\n",
      "        string2 (str): second string\n",
      "        idfsDictionary (dictionary): a dictionary of IDF values\n",
      "    Returns:\n",
      "        cossim: cosine similarity value\n",
      "    \"\"\"\n",
      "    w1 = tfidf(string1.split() ,idfsDictionary)\n",
      "    w2 = tfidf(string2.split() ,idfsDictionary)\n",
      "    return cossim(w1, w2)\n",
      "\n",
      "cossimAdobe = cosineSimilarity('Adobe Photoshop',\n",
      "                               'Adobe Illustrator',\n",
      "                               idfsSmallWeights)\n",
      "\n",
      "print cossimAdobe\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "KeyError",
       "evalue": "'Photoshop'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-111-bd44237e1f75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m cossimAdobe = cosineSimilarity('Adobe Photoshop',\n\u001b[1;32m     15\u001b[0m                                \u001b[0;34m'Adobe Illustrator'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                                idfsSmallWeights)\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mcossimAdobe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-111-bd44237e1f75>\u001b[0m in \u001b[0;36mcosineSimilarity\u001b[0;34m(string1, string2, idfsDictionary)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mw1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0midfsDictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mw2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0midfsDictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcossim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m cossimAdobe = cosineSimilarity('Adobe Photoshop',\n",
        "\u001b[0;32m<ipython-input-106-be622d4fd845>\u001b[0m in \u001b[0;36mcossim\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mthen\u001b[0m \u001b[0mby\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnorm\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msecond\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \"\"\"\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdotprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mtestVec1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'foo'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bar'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'baz'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-106-be622d4fd845>\u001b[0m in \u001b[0;36mdotprod\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mdotProd\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresult\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdot\u001b[0m \u001b[0mproduct\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtwo\u001b[0m \u001b[0minput\u001b[0m \u001b[0mdictionaries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \"\"\"\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/brenden/.local/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   1707\u001b[0m     \"\"\"\n\u001b[1;32m   1708\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_gentype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1709\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sum_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1710\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m             \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-106-be622d4fd845>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m((k,))\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mdotProd\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresult\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdot\u001b[0m \u001b[0mproduct\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtwo\u001b[0m \u001b[0minput\u001b[0m \u001b[0mdictionaries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \"\"\"\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyError\u001b[0m: 'Photoshop'"
       ]
      }
     ],
     "prompt_number": 111
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}